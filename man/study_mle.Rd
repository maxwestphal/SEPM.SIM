% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/study_mle.R
\name{study_mle}
\alias{study_mle}
\title{Select models based on validation performance and conduct an evaluation study}
\usage{
study_mle(
  instance,
  methods = NA,
  M = 200,
  M.start = NA,
  M.probs = c("uniform", "learn"),
  M.seed = 1,
  n.eval = 200,
  first.eval = 1,
  rdm.eval = FALSE,
  analysis = c("acc", "cpe"),
  delta = 0,
  shift = 0.05,
  select.method = c("close", "best", "optimal", "oracle", "simplest.en"),
  select.limit = c("none", "sqrt", "one"),
  select.args = "",
  estimate.method = "beta.approx",
  estimate.args = "",
  infer.method = "maxT",
  alternative = "greater",
  alpha = 0.025,
  transform = "none",
  data = NULL,
  job = NULL
)
}
\arguments{
\item{instance}{simulation instance generated by \code{sample_mle}}

\item{methods}{character, potentially subset available prediction models by method (=learning algorithm)
e.g. recover elastic net models by specifying methods="glmnet" (caret train.method),
no effect if methods=NA (default)}

\item{M}{integer, number of models to subsample from available models (restricted via methods argument),
needs to be less or euqal than number of available models (200 per default)}

\item{M.start}{integer, starting index for subsetting}

\item{M.probs}{character, "uniform" for random subset, "learn.theta" for P(selected)=learn.theta(=true model performance),
"learn.theta.neg" for P(selected)=1-learn.theta}

\item{M.seed}{integer, seed for random subsetting (i.e. if M.probs != "uniform")}

\item{n.eval}{integer, test (evaluation) sample size}

\item{first.eval}{integer, index of first evaluation observation (from all available)}

\item{rdm.eval}{logical, choose test samples randomly? (default: FALSE)}

\item{analysis}{character, either "acc" or "cpe"}

\item{delta}{numeric (default: 0)}

\item{shift}{numeric (default: 0.05)}

\item{select.method}{character, selection method based on validation ranking, e.g. "rank" (default) or "se"}

\item{select.limit}{integer, maximum number of models to evaluate}

\item{select.args}{character, further arguments defining selection rule e.g. "r=1" for
select.method="rank" to choose only best validation models or "c=1" for select.method="se"
(which defines the 'within1SE# rule)}

\item{estimate.method}{character, estimation method in SEPM package default ("beta.approx")}

\item{estimate.args}{character, specify additional estimation argument as character of form
"arg1=value1_arg2=value2_..."}

\item{infer.method}{character, defines the statistical test, e.g. "maxT", "Bonferroni" or "naive"}

\item{alternative}{character, either "greater" (default), "lower" or "two.sided"}

\item{alpha}{numeric, significance level (default: 0.025)}

\item{transform}{character, specifies transformation of test statistics, passed to \code{SEPM::infer}}

\item{data}{ignored (required for batchtools compatibility)}

\item{job}{ignored (required for batchtools compatibility)}
}
\value{
Returns a list which contains all relevant characteristics of the evaluation study.
}
\description{
This function reads in data instances produces via sample_mle() and emulates the process of
conducting an evaluation study for one or multiple selected prediction models.
}
